{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Packages\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import importlib.util\n",
    "from typing import *\n",
    "from tqdm import tqdm \n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from JS_Architects import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We will use Architects model\n",
    "\n",
    "1) Implement Custom pipeline, no prompting just prediction with shrekking of the embedding \n",
    "\n",
    "2) Automated Forsequence Classification\n",
    "    1) No prompting\n",
    "    2) Prompt: \"Classify with labels encode in binary\"\n",
    "    3) Prompt: \"Clasify with list label: [\"depth\",...]\"\n",
    "    4) Prompt \"Classify with list label: [\"depth\",...]. Depth represents..., Containment represents....\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Pipeline \n",
    "\n",
    "## 1. Prepare Data\n",
    "\n",
    "### Load \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"perceptions_training.json\", \"r\") as f:\n",
    "    dic_training = json.load(f)\n",
    "\n",
    "with open(\"perceptions_testing.json\", \"r\") as f:\n",
    "    dic_testing = json.load(f)\n",
    "\n",
    "label_list = [\n",
    "    \"containment\",\n",
    "    \"depth\",\n",
    "    \"symmetry\",\n",
    "    \"categorical\",\n",
    "    \"spatial-Orientation\",\n",
    "    \"spatial-Ordinal\",\n",
    "    \"similarity\",\n",
    "    \"quantitative\",\n",
    "    \"replication\",\n",
    "    \"figure-Ground\",\n",
    "    \"continuity\",\n",
    "    \"size\",\n",
    "    \"closure\",\n",
    "    \"centroid\",\n",
    "    \"topological\",\n",
    "    \"motion\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_data_for_multilabel_classification(\n",
    "    dic_training,\n",
    "    instruction,\n",
    "    label_list,\n",
    "    inp_prefix=\"<I>\",\n",
    "    out_prefix=\"<O>\",\n",
    "    arr_sep=\"\\n\",\n",
    "    exa_sep=\"\\n---\\n\",\n",
    "    bos_token=\"<|begin_of_text|>\",\n",
    "    eos_token=\"<|end_of_text|>\"\n",
    "):\n",
    "    llama_data = []\n",
    "\n",
    "    # Create a mapping from label to index\n",
    "    label_to_index = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    for entry_id, content in dic_training.items():\n",
    "        # Extract perceptions (labels) and encode them as a binary vector\n",
    "        perceptions = content.get(\"perceptions\", [])\n",
    "        label_vector = labels_to_binary(label_list, perceptions)\n",
    "\n",
    "        # Combine train and test examples\n",
    "        examples = content.get(\"example\", {}).get(\"train\", []) + content.get(\"example\", {}).get(\"test\", [])\n",
    "\n",
    "        # Format examples into a single input string\n",
    "        formatted_examples = []\n",
    "        for example in examples:\n",
    "            input_data = f\"{inp_prefix}{format_array(example['input'], arr_sep)}\"\n",
    "            output_data = f\"{out_prefix}{format_array(example['output'], arr_sep)}{eos_token}\"\n",
    "            formatted_examples.append(f\"{input_data}{exa_sep}{output_data}\")\n",
    "\n",
    "        # Combine all examples into one input text and prepend the BOS token\n",
    "        combined_text = f\"{exa_sep.join(formatted_examples)}\"\n",
    "\n",
    "        # Add the structured data for fine-tuning\n",
    "        llama_data.append({\n",
    "            \"instruction\": f\"{instruction}\",\n",
    "            \"input\": combined_text,\n",
    "            \"output\": label_vector,  # Multi-label as binary vector\n",
    "        })\n",
    "\n",
    "    return llama_data\n",
    "\n",
    "def format_array(array, arr_sep=\"\\n\"):\n",
    "    \"\"\"\n",
    "    Helper function to format a 2D array into a string with row-wise separation.\n",
    "    \"\"\"\n",
    "    return arr_sep.join([\" \".join(map(str, row)) for row in array])\n",
    "\n",
    "def labels_to_binary(label_list, input_labels):\n",
    "    \"\"\"\n",
    "    Convert perceptions into a binary vector based on the label list.\n",
    "    Handles both single strings and lists of strings for input_labels.\n",
    "    \"\"\"\n",
    "    # Ensure input_labels is treated as a list\n",
    "    if isinstance(input_labels, str):\n",
    "        input_labels = [input_labels]\n",
    "    \n",
    "    # Create a set of lowercase input labels\n",
    "    input_set = set(label.lower() for label in input_labels)\n",
    "    \n",
    "    # Generate the binary vector\n",
    "    return [1 if label.lower() in input_set else 0 for label in label_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction0 = \"\"\n",
    "instruction1 = \"Classify the relationship between the input and output sequences based on perceptions\"\n",
    "\n",
    "# List\n",
    "llama_data = prepare_data_for_multilabel_classification(dic_training,instruction0,label_list)\n",
    "\n",
    "# Dict\n",
    "llama_data_dict = {\n",
    "    \"instruction\": [item[\"instruction\"] for item in llama_data],\n",
    "    \"input\": [item[\"input\"] for item in llama_data],\n",
    "    \"output\": [item[\"output\"] for item in llama_data],\n",
    "}\n",
    "\n",
    "# Dataset\n",
    "llama_data_dataset = Dataset.from_dict(llama_data_dict)\n",
    "\n",
    "llama_data_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_data_list_testing = prepare_data_for_multilabel_classification(dic_testing,instruction0, label_list)\n",
    "\n",
    "# Restructure llama_data\n",
    "llama_data_dict_testing = {\n",
    "    \"instruction\": [item[\"instruction\"] for item in llama_data_list_testing],\n",
    "    \"input\": [item[\"input\"] for item in llama_data_list_testing],\n",
    "    \"output\": [item[\"output\"] for item in llama_data_list_testing],\n",
    "}\n",
    "\n",
    "llama_data_dataset_testing = Dataset.from_dict(llama_data_dict_testing)\n",
    "llama_data_dataset_testing[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Install bitsandbytes if not already installed\n",
    "# !pip install bitsandbytes\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "\n",
    "print(\"Predefined Special Tokens:\")\n",
    "print(f\"EOS Token: {tokenizer.bos_token}, ID: {tokenizer.bos_token_id}\")\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shrink the tokenizer and embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "    \"input\": \"<I>\",\n",
    "    \"output\": \"<O>\",\n",
    "    \"array_sep\": \"\\n\",\n",
    "    \"example_sep\": \"\\n---\\n\",\n",
    "    \"eos_token\": \"<|end_of_text|>\",\n",
    "    \"bos_token\": \"<|begin_of_text|>\",\n",
    "    \"pad_token\": \"[PAD]\"\n",
    "}\n",
    "\n",
    "# Add special tokens\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        special_tokens_dict[\"input\"],\n",
    "        special_tokens_dict[\"output\"],\n",
    "        special_tokens_dict[\"array_sep\"],\n",
    "        special_tokens_dict[\"example_sep\"]\n",
    "    ],\n",
    "    \"eos_token\": special_tokens_dict[\"eos_token\"],\n",
    "    \"bos_token\": special_tokens_dict[\"bos_token\"],\n",
    "    \"pad_token\": special_tokens_dict[\"pad_token\"]\n",
    "})\n",
    "\n",
    "# Set the tokenizer pad token explicitly\n",
    "tokenizer.pad_token = special_tokens_dict[\"pad_token\"]\n",
    "\n",
    "# Check the updated tokens\n",
    "print(f\"Special Tokens: {tokenizer.special_tokens_map}\")\n",
    "print(f\"Vocabulary Size: {len(tokenizer)}\")\n",
    "\n",
    "# Resize model embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus_for_shrinking(hf_dataset):\n",
    "    \"\"\"\n",
    "    Concatenate the 'input' + 'instruction' from the dataset\n",
    "    to ensure all relevant tokens appear.\n",
    "    \"\"\"\n",
    "    corpus_list = []\n",
    "    for sample in hf_dataset:\n",
    "        text = (sample[\"instruction\"] or \"\") + \" \" + (sample[\"input\"] or \"\")\n",
    "        corpus_list.append(text)\n",
    "    # Combine into one big string\n",
    "    corpus = \"\\n\".join(corpus_list)\n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus_for_shrinking(llama_data_dataset)\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shrink_embeddings(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            corpus=corpus,                  # ensures relevant tokens are kept\n",
    "            keep_special_tokens=True,\n",
    "            keep_normalizer=False,\n",
    "            keep_token_order=True\n",
    "        )\n",
    "\n",
    "print(\"Tokenizer size after shrinking:\", len(tokenizer.vocab))\n",
    "\n",
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence = llama_data_dataset[0][\"input\"]\n",
    "\n",
    "encoded = tokenizer.encode(test_sequence)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply LoRA to the Shrunk Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the shrunk model with LoRA\n",
    "model_shrinked = get_peft_model(model, peft_config)\n",
    "\n",
    "# Make sure to unfreeze embeddings if you want to train them directly \n",
    "# (LoRA on embed_tokens will still add ranks; but if you want the base embedding \n",
    "#  weights to be trainable, do something like):\n",
    "for param in model_shrinked.get_input_embeddings().parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full architecture\n",
    "\n",
    "### Add Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "import torch.nn as nn\n",
    "\n",
    "class LLMWithClassificationHead(PreTrainedModel):\n",
    "    def __init__(self, base_model, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        if isinstance(base_model, LLMWithClassificationHead):\n",
    "            raise ValueError(\"base_model cannot be an instance of LLMWithClassificationHead\")\n",
    "        \n",
    "        self.base_model_1 = base_model\n",
    "        self.num_labels = num_labels\n",
    "        hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "    def forward(self, input_ids, output_hidden_states=True, return_dict=True, attention_mask=None, labels=None):\n",
    "        outputs = self.base_model_1(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "        pooled = last_hidden[:, -1, :]  # Taking the last token's hidden state\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"loss\": loss,\n",
    "            \"hidden_states\": outputs.hidden_states,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "# Load configuration and base model\n",
    "base_config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "\n",
    "# Define number of labels for classification\n",
    "num_labels = 16\n",
    "\n",
    "# Instantiate the custom model\n",
    "model_classification = LLMWithClassificationHead(\n",
    "    base_model=model,\n",
    "    config=base_config,\n",
    "    num_labels=num_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(tokenizer.encode(example[\"input\"])) for example in llama_data_dataset]\n",
    "print(\"Max length:\", max(lengths), \"95th percentile:\", np.percentile(lengths, 95))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=7000  # Reduced from 8192 to 2048\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": examples[\"output\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# Keep only the input output no instructions for now\n",
    "final_dataset = llama_data_dataset.remove_columns([\"instruction\"])\n",
    "final_dataset_testing = llama_data_dataset_testing.remove_columns([\"instruction\"])\n",
    "\n",
    "tokenized_final_dataset = final_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_final_test = final_dataset_testing.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Collector\n",
    "\n",
    "class DataCollatorWithLabels:\n",
    "    def __call__(self, features):\n",
    "        input_ids = torch.tensor([f[\"input_ids\"] for f in features], dtype=torch.long)\n",
    "        attention_mask = torch.tensor([f[\"attention_mask\"] for f in features], dtype=torch.long)\n",
    "        labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float32)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "data_collator = DataCollatorWithLabels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Extract labels\n",
    "        labels = inputs.pop(\"labels\", None)\n",
    "        if labels is None:\n",
    "            raise ValueError(\"Labels are missing in inputs\")\n",
    "\n",
    "        \n",
    "        # Forward pass with additional arguments\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        \n",
    "        # Extract loss\n",
    "        loss = outputs[\"loss\"]\n",
    "        \n",
    "        # Return loss and outputs if required\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Train - Optimized\n",
    "\n",
    "# %%\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear cache before training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define TrainingArguments with optimizations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./JS_finetuned_model\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=1e-4,  # instead of 1e-4\n",
    "    per_device_train_batch_size=1,  # Reduced batch size\n",
    "    per_device_eval_batch_size=1,  # Reduced batch size\n",
    "    num_train_epochs=3,  # Increased epochs if feasible\n",
    "    fp16=False,\n",
    "    gradient_accumulation_steps=8,  # Increased gradient accumulation\n",
    "    #gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "    save_total_limit=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"wandb\",  # Log to W&B\n",
    "    # Add any other necessary arguments\n",
    ")\n",
    "\n",
    "# Use built-in data collator with dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding='longest')\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model_classification,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_final_dataset,\n",
    "    eval_dataset=tokenized_final_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer.train()\n",
    "\n",
    "# Clear memory after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir=\"working/models\",safe_serialization=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = tokenized_final_test.select(range(10))\n",
    "\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run predictions on the evaluation dataset\n",
    "predictions = trainer.predict(tokenized_final_test)\n",
    "\n",
    "# Extract logits, true labels, and metrics\n",
    "logits = predictions.predictions  # Model outputs\n",
    "true_labels = predictions.label_ids  # True labels from the dataset\n",
    "metrics = predictions.metrics  # Evaluation metrics\n",
    "\n",
    "# Print metrics\n",
    "print(\"Evaluation Metrics:\", metrics)\n",
    "\n",
    "# Clear memory after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I find weird is that it has two outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(logits[0])):\n",
    "    exp_scores = np.exp(logits[0][i])\n",
    "    probabilities = exp_scores / np.sum(exp_scores)\n",
    "    pred_label_indices = np.argsort(probabilities)[-2:]  # get 2 largest\n",
    "    pred_label_indices\n",
    "    print(f\"Example {i}\")\n",
    "    print(\"Predicted Labels:\", [label_list[idx] for idx in pred_label_indices])\n",
    "    print(\"True Labels:\", [label_list[idx] for idx, val in enumerate(true_labels[i]) if val == 1])\n",
    "    \n",
    "    #print(\"True Labels:\", true_labels[i])\n",
    "    #print(\"Probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores = np.exp(logits[0][i])\n",
    "probabilities = exp_scores / np.sum(exp_scores)\n",
    "probabilities\n",
    "predicted_label_index = np.argmax(probabilities)\n",
    "predicted_label_index\n",
    "\n",
    "pred_label_indices = np.argsort(probabilities)[-2:]  # get 2 largest\n",
    "pred_label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Convert to PyTorch tensors and pad\n",
    "logits_padded = pad_sequence([torch.tensor(logit) for logit in logits], batch_first=True)\n",
    "print(\"Padded logits shape:\", logits_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Logits:\", logits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
