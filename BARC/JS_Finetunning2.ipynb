{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdelinea/anaconda3/envs/BarcHandbook/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Required Packages\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import importlib.util\n",
    "from typing import *\n",
    "from tqdm import tqdm \n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from JS_Architects import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We will use Architects model\n",
    "\n",
    "1) Implement Custom pipeline, no prompting just prediction with shrekking of the embedding \n",
    "\n",
    "2) Automated Forsequence Classification\n",
    "    1) No prompting\n",
    "    2) Prompt: \"Classify with labels encode in binary\"\n",
    "    3) Prompt: \"Clasify with list label: [\"depth\",...]\"\n",
    "    4) Prompt \"Classify with list label: [\"depth\",...]. Depth represents..., Containment represents....\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Pipeline \n",
    "\n",
    "## 1. Prepare Data\n",
    "\n",
    "### Load \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"perceptions_training.json\", \"r\") as f:\n",
    "    dic_training = json.load(f)\n",
    "\n",
    "with open(\"perceptions_testing.json\", \"r\") as f:\n",
    "    dic_testing = json.load(f)\n",
    "\n",
    "label_list = [\n",
    "    \"containment\",\n",
    "    \"depth\",\n",
    "    \"symmetry\",\n",
    "    \"categorical\",\n",
    "    \"spatial-Orientation\",\n",
    "    \"spatial-Ordinal\",\n",
    "    \"similarity\",\n",
    "    \"quantitative\",\n",
    "    \"replication\",\n",
    "    \"figure-Ground\",\n",
    "    \"continuity\",\n",
    "    \"size\",\n",
    "    \"closure\",\n",
    "    \"centroid\",\n",
    "    \"topological\",\n",
    "    \"motion\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_data_for_multilabel_classification(\n",
    "    dic_training,\n",
    "    instruction,\n",
    "    label_list,\n",
    "    inp_prefix=\"<I>\",\n",
    "    out_prefix=\"<O>\",\n",
    "    arr_sep=\"\\n\",\n",
    "    exa_sep=\"\\n---\\n\",\n",
    "    bos_token=\"<|begin_of_text|>\",\n",
    "    eos_token=\"<|end_of_text|>\"\n",
    "):\n",
    "    llama_data = []\n",
    "\n",
    "    # Create a mapping from label to index\n",
    "    label_to_index = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    for entry_id, content in dic_training.items():\n",
    "        # Extract perceptions (labels) and encode them as a binary vector\n",
    "        perceptions = content.get(\"perceptions\", [])\n",
    "        label_vector = labels_to_binary(label_list, perceptions)\n",
    "\n",
    "        # Combine train and test examples\n",
    "        examples = content.get(\"example\", {}).get(\"train\", []) + content.get(\"example\", {}).get(\"test\", [])\n",
    "\n",
    "        # Format examples into a single input string\n",
    "        formatted_examples = []\n",
    "        for example in examples:\n",
    "            input_data = f\"{inp_prefix}{format_array(example['input'], arr_sep)}\"\n",
    "            output_data = f\"{out_prefix}{format_array(example['output'], arr_sep)}{eos_token}\"\n",
    "            formatted_examples.append(f\"{input_data}{exa_sep}{output_data}\")\n",
    "\n",
    "        # Combine all examples into one input text and prepend the BOS token\n",
    "        combined_text = f\"{exa_sep.join(formatted_examples)}\"\n",
    "\n",
    "        # Add the structured data for fine-tuning\n",
    "        llama_data.append({\n",
    "            \"instruction\": f\"{instruction}\",\n",
    "            \"input\": combined_text,\n",
    "            \"output\": label_vector,  # Multi-label as binary vector\n",
    "        })\n",
    "\n",
    "    return llama_data\n",
    "\n",
    "def format_array(array, arr_sep=\"\\n\"):\n",
    "    \"\"\"\n",
    "    Helper function to format a 2D array into a string with row-wise separation.\n",
    "    \"\"\"\n",
    "    return arr_sep.join([\" \".join(map(str, row)) for row in array])\n",
    "\n",
    "def labels_to_binary(label_list, input_labels):\n",
    "    \"\"\"\n",
    "    Convert perceptions into a binary vector based on the label list.\n",
    "    Handles both single strings and lists of strings for input_labels.\n",
    "    \"\"\"\n",
    "    # Ensure input_labels is treated as a list\n",
    "    if isinstance(input_labels, str):\n",
    "        input_labels = [input_labels]\n",
    "    \n",
    "    # Create a set of lowercase input labels\n",
    "    input_set = set(label.lower() for label in input_labels)\n",
    "    \n",
    "    # Generate the binary vector\n",
    "    return [1 if label.lower() in input_set else 0 for label in label_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '',\n",
       " 'input': '<I>0 7 7\\n7 7 7\\n0 7 7\\n---\\n<O>0 0 0 0 7 7 0 7 7\\n0 0 0 7 7 7 7 7 7\\n0 0 0 0 7 7 0 7 7\\n0 7 7 0 7 7 0 7 7\\n7 7 7 7 7 7 7 7 7\\n0 7 7 0 7 7 0 7 7\\n0 0 0 0 7 7 0 7 7\\n0 0 0 7 7 7 7 7 7\\n0 0 0 0 7 7 0 7 7<|end_of_text|>\\n---\\n<I>4 0 4\\n0 0 0\\n0 4 0\\n---\\n<O>4 0 4 0 0 0 4 0 4\\n0 0 0 0 0 0 0 0 0\\n0 4 0 0 0 0 0 4 0\\n0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0\\n0 0 0 4 0 4 0 0 0\\n0 0 0 0 0 0 0 0 0\\n0 0 0 0 4 0 0 0 0<|end_of_text|>\\n---\\n<I>0 0 0\\n0 0 2\\n2 0 2\\n---\\n<O>0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 2\\n0 0 0 0 0 0 2 0 2\\n0 0 0 0 0 0 0 0 0\\n0 0 2 0 0 0 0 0 2\\n2 0 2 0 0 0 2 0 2<|end_of_text|>\\n---\\n<I>6 6 0\\n6 0 0\\n0 6 6\\n---\\n<O>6 6 0 6 6 0 0 0 0\\n6 0 0 6 0 0 0 0 0\\n0 6 6 0 6 6 0 0 0\\n6 6 0 0 0 0 0 0 0\\n6 0 0 0 0 0 0 0 0\\n0 6 6 0 0 0 0 0 0\\n0 0 0 6 6 0 6 6 0\\n0 0 0 6 0 0 6 0 0\\n0 0 0 0 6 6 0 6 6<|end_of_text|>\\n---\\n<I>2 2 2\\n0 0 0\\n0 2 2\\n---\\n<O>2 2 2 2 2 2 2 2 2\\n0 0 0 0 0 0 0 0 0\\n0 2 2 0 2 2 0 2 2\\n0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0\\n0 0 0 0 0 0 0 0 0\\n0 0 0 2 2 2 2 2 2\\n0 0 0 0 0 0 0 0 0\\n0 0 0 0 2 2 0 2 2<|end_of_text|>\\n---\\n<I>7 0 7\\n7 0 7\\n7 7 0\\n---\\n<O>7 0 7 0 0 0 7 0 7\\n7 0 7 0 0 0 7 0 7\\n7 7 0 0 0 0 7 7 0\\n7 0 7 0 0 0 7 0 7\\n7 0 7 0 0 0 7 0 7\\n7 7 0 0 0 0 7 7 0\\n7 0 7 7 0 7 0 0 0\\n7 0 7 7 0 7 0 0 0\\n7 7 0 7 7 0 0 0 0<|end_of_text|>',\n",
       " 'output': [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction0 = \"\"\n",
    "instruction1 = \"Classify the relationship between the input and output sequences based on perceptions\"\n",
    "\n",
    "# List\n",
    "llama_data = prepare_data_for_multilabel_classification(dic_training,instruction0,label_list)\n",
    "\n",
    "# Dict\n",
    "llama_data_dict = {\n",
    "    \"instruction\": [item[\"instruction\"] for item in llama_data],\n",
    "    \"input\": [item[\"input\"] for item in llama_data],\n",
    "    \"output\": [item[\"output\"] for item in llama_data],\n",
    "}\n",
    "\n",
    "# Dataset\n",
    "llama_data_dataset = Dataset.from_dict(llama_data_dict)\n",
    "\n",
    "llama_data_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '',\n",
       " 'input': '<I>8 6\\n6 4\\n---\\n<O>8 6 8 6 8 6\\n6 4 6 4 6 4\\n6 8 6 8 6 8\\n4 6 4 6 4 6\\n8 6 8 6 8 6\\n6 4 6 4 6 4<|end_of_text|>\\n---\\n<I>7 9\\n4 3\\n---\\n<O>7 9 7 9 7 9\\n4 3 4 3 4 3\\n9 7 9 7 9 7\\n3 4 3 4 3 4\\n7 9 7 9 7 9\\n4 3 4 3 4 3<|end_of_text|>\\n---\\n<I>3 2\\n7 8\\n---\\n<O>3 2 3 2 3 2\\n7 8 7 8 7 8\\n2 3 2 3 2 3\\n8 7 8 7 8 7\\n3 2 3 2 3 2\\n7 8 7 8 7 8<|end_of_text|>',\n",
       " 'output': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_data_list_testing = prepare_data_for_multilabel_classification(dic_testing,instruction0, label_list)\n",
    "\n",
    "# Restructure llama_data\n",
    "llama_data_dict_testing = {\n",
    "    \"instruction\": [item[\"instruction\"] for item in llama_data_list_testing],\n",
    "    \"input\": [item[\"input\"] for item in llama_data_list_testing],\n",
    "    \"output\": [item[\"output\"] for item in llama_data_list_testing],\n",
    "}\n",
    "\n",
    "llama_data_dataset_testing = Dataset.from_dict(llama_data_dict_testing)\n",
    "llama_data_dataset_testing[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predefined Special Tokens:\n",
      "EOS Token: <|begin_of_text|>, ID: 128000\n",
      "128256\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Install bitsandbytes if not already installed\n",
    "# !pip install bitsandbytes\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "# Load quantized model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    load_in_4bit=True,  # Use 4-bit quantization\n",
    "    device_map=\"auto\",\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Match input type\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Predefined Special Tokens:\")\n",
    "print(f\"EOS Token: {tokenizer.bos_token}, ID: {tokenizer.bos_token_id}\")\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shrink the tokenizer and embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>', 'pad_token': '[PAD]', 'additional_special_tokens': ['<I>', '<O>', '\\n', '\\n---\\n']}\n",
      "Vocabulary Size: 128261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128261, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {\n",
    "    \"input\": \"<I>\",\n",
    "    \"output\": \"<O>\",\n",
    "    \"array_sep\": \"\\n\",\n",
    "    \"example_sep\": \"\\n---\\n\",\n",
    "    \"eos_token\": \"<|end_of_text|>\",\n",
    "    \"bos_token\": \"<|begin_of_text|>\",\n",
    "    \"pad_token\": \"[PAD]\"\n",
    "}\n",
    "\n",
    "# Add special tokens\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        special_tokens_dict[\"input\"],\n",
    "        special_tokens_dict[\"output\"],\n",
    "        special_tokens_dict[\"array_sep\"],\n",
    "        special_tokens_dict[\"example_sep\"]\n",
    "    ],\n",
    "    \"eos_token\": special_tokens_dict[\"eos_token\"],\n",
    "    \"bos_token\": special_tokens_dict[\"bos_token\"],\n",
    "    \"pad_token\": special_tokens_dict[\"pad_token\"]\n",
    "})\n",
    "\n",
    "# Set the tokenizer pad token explicitly\n",
    "tokenizer.pad_token = special_tokens_dict[\"pad_token\"]\n",
    "\n",
    "# Check the updated tokens\n",
    "print(f\"Special Tokens: {tokenizer.special_tokens_map}\")\n",
    "print(f\"Vocabulary Size: {len(tokenizer)}\")\n",
    "\n",
    "# Resize model embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382576"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_corpus_for_shrinking(hf_dataset):\n",
    "    \"\"\"\n",
    "    Concatenate the 'input' + 'instruction' from the dataset\n",
    "    to ensure all relevant tokens appear.\n",
    "    \"\"\"\n",
    "    corpus_list = []\n",
    "    for sample in hf_dataset:\n",
    "        text = (sample[\"instruction\"] or \"\") + \" \" + (sample[\"input\"] or \"\")\n",
    "        corpus_list.append(text)\n",
    "    # Combine into one big string\n",
    "    corpus = \"\\n\".join(corpus_list)\n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus_for_shrinking(llama_data_dataset)\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (366563 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer size after shrinking: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 17,\n",
       " '3': 3,\n",
       " '0': 0,\n",
       " '6': 6,\n",
       " 'Ġ': 10,\n",
       " '<|begin_of_text|>': 11,\n",
       " '8': 8,\n",
       " '<|end_of_text|>': 12,\n",
       " '2': 2,\n",
       " '4': 4,\n",
       " '<I>': 13,\n",
       " '7': 7,\n",
       " '\\n---\\n': 16,\n",
       " '5': 5,\n",
       " '\\n': 15,\n",
       " '9': 9,\n",
       " '1': 1,\n",
       " '<O>': 14}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shrink_embeddings(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            corpus=corpus,                  # ensures relevant tokens are kept\n",
    "            keep_special_tokens=True,\n",
    "            keep_normalizer=False,\n",
    "            keep_token_order=True\n",
    "        )\n",
    "\n",
    "print(\"Tokenizer size after shrinking:\", len(tokenizer.vocab))\n",
    "\n",
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [11, 13, 8, 10, 6, 15, 6, 10, 4, 16, 14, 8, 10, 6, 10, 8, 10, 6, 10, 8, 10, 6, 15, 6, 10, 4, 10, 6, 10, 4, 10, 6, 10, 4, 15, 6, 10, 8, 10, 6, 10, 8, 10, 6, 10, 8, 15, 4, 10, 6, 10, 4, 10, 6, 10, 4, 10, 6, 15, 8, 10, 6, 10, 8, 10, 6, 10, 8, 10, 6, 15, 6, 10, 4, 10, 6, 10, 4, 10, 6, 10, 4, 12, 16, 13, 7, 10, 9, 15, 4, 10, 3, 16, 14, 7, 10, 9, 10, 7, 10, 9, 10, 7, 10, 9, 15, 4, 10, 3, 10, 4, 10, 3, 10, 4, 10, 3, 15, 9, 10, 7, 10, 9, 10, 7, 10, 9, 10, 7, 15, 3, 10, 4, 10, 3, 10, 4, 10, 3, 10, 4, 15, 7, 10, 9, 10, 7, 10, 9, 10, 7, 10, 9, 15, 4, 10, 3, 10, 4, 10, 3, 10, 4, 10, 3, 12, 16, 13, 3, 10, 2, 15, 7, 10, 8, 16, 14, 3, 10, 2, 10, 3, 10, 2, 10, 3, 10, 2, 15, 7, 10, 8, 10, 7, 10, 8, 10, 7, 10, 8, 15, 2, 10, 3, 10, 2, 10, 3, 10, 2, 10, 3, 15, 8, 10, 7, 10, 8, 10, 7, 10, 8, 10, 7, 15, 3, 10, 2, 10, 3, 10, 2, 10, 3, 10, 2, 15, 7, 10, 8, 10, 7, 10, 8, 10, 7, 10, 8, 12]\n",
      "Decoded: <|begin_of_text|><I>8 6\n",
      "6 4\n",
      "---\n",
      "<O>8 6 8 6 8 6\n",
      "6 4 6 4 6 4\n",
      "6 8 6 8 6 8\n",
      "4 6 4 6 4 6\n",
      "8 6 8 6 8 6\n",
      "6 4 6 4 6 4<|end_of_text|>\n",
      "---\n",
      "<I>7 9\n",
      "4 3\n",
      "---\n",
      "<O>7 9 7 9 7 9\n",
      "4 3 4 3 4 3\n",
      "9 7 9 7 9 7\n",
      "3 4 3 4 3 4\n",
      "7 9 7 9 7 9\n",
      "4 3 4 3 4 3<|end_of_text|>\n",
      "---\n",
      "<I>3 2\n",
      "7 8\n",
      "---\n",
      "<O>3 2 3 2 3 2\n",
      "7 8 7 8 7 8\n",
      "2 3 2 3 2 3\n",
      "8 7 8 7 8 7\n",
      "3 2 3 2 3 2\n",
      "7 8 7 8 7 8<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "test_sequence = \"<I>8 6\\n6 4\\n---\\n<O>8 6 8 6 8 6\\n6 4 6 4 6 4\\n6 8 6 8 6 8\\n4 6 4 6 4 6\\n8 6 8 6 8 6\\n6 4 6 4 6 4<|end_of_text|>\\n---\\n<I>7 9\\n4 3\\n---\\n<O>7 9 7 9 7 9\\n4 3 4 3 4 3\\n9 7 9 7 9 7\\n3 4 3 4 3 4\\n7 9 7 9 7 9\\n4 3 4 3 4 3<|end_of_text|>\\n---\\n<I>3 2\\n7 8\\n---\\n<O>3 2 3 2 3 2\\n7 8 7 8 7 8\\n2 3 2 3 2 3\\n8 7 8 7 8 7\\n3 2 3 2 3 2\\n7 8 7 8 7 8<|end_of_text|>\"\n",
    "encoded = tokenizer.encode(test_sequence)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply LoRA to the Shrunk Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the shrunk model with LoRA\n",
    "model_shrinked = get_peft_model(model, peft_config)\n",
    "\n",
    "# Make sure to unfreeze embeddings if you want to train them directly \n",
    "# (LoRA on embed_tokens will still add ranks; but if you want the base embedding \n",
    "#  weights to be trainable, do something like):\n",
    "for param in model_shrinked.get_input_embeddings().parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full architecture\n",
    "\n",
    "### Add Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "\n",
    "class LLMWithClassificationHead(PreTrainedModel):\n",
    "    def __init__(self, base_model, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.base_model = base_model\n",
    "        self.num_labels = num_labels\n",
    "        hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "        pooled = last_hidden[:, -1, :]  # Taking the last token's hidden state\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"loss\": loss,\n",
    "            \"hidden_states\": outputs.hidden_states,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, AutoConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "class LLMWithClassificationHead(PreTrainedModel):\n",
    "    def __init__(self, base_model, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.base_model = base_model\n",
    "        self.num_labels = num_labels\n",
    "        hidden_size = config.hidden_size\n",
    "\n",
    "        # Define a classification head\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "        # Initialize weights and other components\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Forward pass through the base model\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last_hidden = outputs.hidden_states[-1]  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Pooling: take the last token's hidden state\n",
    "        pooled = last_hidden[:, -1, :]  # (batch_size, hidden_size)\n",
    "\n",
    "        # Classification head\n",
    "        logits = self.classifier(pooled)  # (batch_size, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "            \"hidden_states\": outputs.hidden_states,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        return self.base_model.config\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: int):\n",
    "        self.base_model.resize_token_embeddings(new_num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Instantiate the custom model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model_classification \u001b[38;5;241m=\u001b[39m \u001b[43mLLMWithClassificationHead\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_shrinked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_labels\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m, in \u001b[0;36mLLMWithClassificationHead.__init__\u001b[0;34m(self, base_model, config, num_labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, base_model, config, num_labels):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m base_model\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m num_labels\n",
      "File \u001b[0;32m~/anaconda3/envs/BarcHandbook/lib/python3.10/site-packages/transformers/modeling_utils.py:1389\u001b[0m, in \u001b[0;36mPreTrainedModel.__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attn_implementation_autoset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1386\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   1387\u001b[0m         config, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mget_default_dtype(), check_device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m     )\n\u001b[0;32m-> 1389\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_or_path \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mname_or_path\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarnings_issued \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/BarcHandbook/lib/python3.10/site-packages/torch/nn/modules/module.py:1788\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1786\u001b[0m     buffers[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1788\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute 'config'"
     ]
    }
   ],
   "source": [
    "# Define the final model\n",
    "from transformers import AutoConfig\n",
    "\n",
    "base_config = AutoConfig.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "num_labels = 16\n",
    "\n",
    "# Instantiate the custom model\n",
    "model_classification = LLMWithClassificationHead(\n",
    "    base_model=model_shrinked,\n",
    "    config=base_config,\n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=2048  # Reduced from 8192 to 2048\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": examples[\"output\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# Keep only the input output no instructions for now\n",
    "final_dataset = llama_data_dataset.remove_columns([\"instruction\"])\n",
    "\n",
    "\n",
    "tokenized_final_dataset = final_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Collector\n",
    "\n",
    "class DataCollatorWithLabels:\n",
    "    def __call__(self, features):\n",
    "        input_ids = torch.tensor([f[\"input_ids\"] for f in features], dtype=torch.long)\n",
    "        attention_mask = torch.tensor([f[\"attention_mask\"] for f in features], dtype=torch.long)\n",
    "        labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float32)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "data_collator = DataCollatorWithLabels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Extract labels\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # Forward pass with additional arguments\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        \n",
    "        # Extract loss\n",
    "        loss = outputs[\"loss\"]\n",
    "        \n",
    "        # Return loss and outputs if required\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Train - Optimized\n",
    "\n",
    "# %%\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear cache before training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define TrainingArguments with optimizations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./JS_finetuned_model\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=1,  # Reduced batch size\n",
    "    num_train_epochs=3,  # Increased epochs if feasible\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=8,  # Increased gradient accumulation\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"tensorboard\",\n",
    "    # Add any other necessary arguments\n",
    ")\n",
    "\n",
    "# Use built-in data collator with dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding='longest')\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model_classification,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_final_dataset,\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer.train()\n",
    "\n",
    "# Clear memory after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your custom model\n",
    "model_Classification = LLMWithClassificationHead(base_model=model, num_labels=16)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    evaluation_strategy=\"steps\",  # Evaluate periodically\n",
    "    eval_steps=500,              # Evaluate every 500 steps\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,  # Reduced\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=True,                    # Mixed-precision\n",
    "    push_to_hub=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = llama_data_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_testing = llama_data_dataset_testing.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithLabels()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BarcHandbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
